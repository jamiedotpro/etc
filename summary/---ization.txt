1. normalization (정규화)
-- MinMax = (x - min) / (max - min)

​normalization은 정규화이다. 조금 헷갈리는게, 표준화와 정규화가 약간 혼용되어 사용된다는 점이다.
일단 구글링의 결과 정규화는 우리가 고등학교때 배운 정규분포의 정규화가 아니라 데이터의 범주를 바꾸는 작업이다.
예를 들어 여러개의 feature가 있는데 어떤거는 100 ~ 200사이의 값이고, 어떤거는 -10 ~ 10 사이의 값이고,
어떤거는 -100 ~ 300 사이의 값이라고 해보자. 그러면 이 값들을 분석하기 쉽지가 않을 것이다.
따라서 이런 불편을 줄이기 위해 범주를 일치시킬 수 있다. 이런 작업을 normalization이라고 한다.
보통은 0 ~ 1 사이의 값으로 범주를 일치시킨다. 밑에 식을 이용해서 normalize하면 0 ~ 1로 범주가 일치된다.


​2. standardization (표준화)
-- Standard = (x - 평균) / 표준편차
x값 : 1, 2, 3, 4, 5
평균 : 3
분산 : -2, -1, 0, 1, 2
        4, 1, 0, 1, 4 => 다 더하면 => 10
        10 / x개수
        분산 == 2
표준편차 : sqrt(분산)
고등학교때 배운 표준정규분포를 구하는 식으로 구하면 된다.
다른 말로는 z - transformation이라고도 하고, 그렇게 표준화된 값을 z - score라고도 한다.
standardization을 통해 KNN에서 해본 wine classification을 해보면 94%정도의 정확도가 나온다.
다시한번 말하지만 물론 KNN말고도 일반적인 데이터 전처리에 사용된다. 그리고 식은 아래와 같다.


3. Regularization (일반화, 정칙화)
과적합이 발생되는 부분에서 패널티를 줘서 해결한다.
l1(랏소) 규제, l2(릿지) 규제 ==> l1은 절대값으로 규제, l2는 제곱해서 규제
Regularization은 W(weight)가 너무 큰 값들을 갖지 않도록 하는 것을 말한다.
값이 커지면, 그림에서 보는 것처럼 구불구불한 형태의 cost 함수가 만들어지고 예측에 실패하게 된다.
머신러닝에서는 "데이터보다 모델의 복잡도(complexity)가 크다"라고 설명한다.
과도하게 복잡하기 때문에 발생하는 문제라고 보는 것이다.
다시 말하면, Regularization은 모델의 복잡도를 낮추기 위한 방법을 말한다.

그럼 과적합(overfitting)문제는 아래와 같은 방법으로 풀 수 있다.
특징의 수를 줄인다.
    - 주요 특징을 직접 선택하고 나머지는 버린다.
    - Model selection algorithm을 사용한다.
Regularization
    - 모든 특징을 사용하나 특징 θj에 대한 parameter를 줄인다.


The Problem of Overfitting
Feature가 너무 많아도 문제가 생긴다. Hypothesis function너무 복잡해지기 때문이다.
이 복잡한 함수는 training set의 데이터 분포를 거의 똑같이 모델링할 수는 있을 것이다.
(J(θ)≈0) 그러나 우리의 목적은 training set과 완벽하게 똑같은 모델을 만드는 게 아니라 training set에 없는 새로운 데이터에 대해서 target을 정확하게 예측하는 것이다.
그러나 training data에 지나치게 맞춰진 모델은 오히려 새로운 데이터를 예측하는 데에는 실패할 수 있다.
이와같이 training data에 지나치게(over) fit 되어 일반적인 추세를 표현하지 못하는 문제를 overfitting이라고 한다.

Adressing Overfitting
그렇다면 overfitting을 피하기 위해 어떻게 할 수 있을까.

Reduce the number of features
일단은 feature를 너무 많이 사용하지 않는 것이 좋다.

사용할 feature를 수동으로 고른다.
Model selection algorithm (later in course)
그러나 feature를 버리기 아까울 수도 있다. 가령, 모든 feature가 조금씩은 traget예측에 도움이 되는 경우가 있다.
이 때에는 feature 갯수를 유지하면서 overfitting을 피하는 방법을 쓴다.
Regularization이라고 부르는 이 방법은 모델이 너무 복잡해지지 않도록 약간의 제약을 가하는 방법이다.

Regularization
-모든 feature를 유지하되, parameter θj 의 크기를 작게 유지한다.
-많은 feature가 y를 예측하는 데에 조금씩 기여하는 경우 유용하다.



* 오버핏의 해결방법
1. Regularization
2. 데이터를 늘린다
3. 피쳐를 줄인다 (피쳐 == 노드. 그래서 나온게 dropout)
    = dropout.